# pytorch_lightning==2.0.0
# pytorch-lightning trainer command line tool

# Set to an int to run seed_everything with this value before classes instantiation.Set to True to use a random seed. (type: Union[bool, int], default: True)
seed_everything: 42

# Customize every aspect of training via flags
trainer:

  # Supports passing different accelerator types ("cpu", "gpu", "tpu", "ipu", "hpu", "mps", "auto")
  # as well as custom accelerator instances. (type: Union[str, Accelerator], default: auto, known subclasses: pytorch_lightning.accelerators.CPUAccelerator, pytorch_lightning.accelerators.CUDAAccelerator, pytorch_lightning.accelerators.HPUAccelerator, pytorch_lightning.accelerators.IPUAccelerator, pytorch_lightning.accelerators.MPSAccelerator, pytorch_lightning.accelerators.TPUAccelerator)
  accelerator: auto

  # Supports different training strategies with aliases as well custom strategies.
  # Default: ``"auto"``. (type: Union[str, Strategy], default: auto, known subclasses: pytorch_lightning.strategies.DDPStrategy, pytorch_lightning.strategies.DeepSpeedStrategy, pytorch_lightning.strategies.HPUParallelStrategy, pytorch_lightning.strategies.XLAStrategy, pytorch_lightning.strategies.FSDPStrategy, pytorch_lightning.strategies.IPUStrategy, pytorch_lightning.strategies.SingleDeviceStrategy, pytorch_lightning.strategies.SingleHPUStrategy, pytorch_lightning.strategies.SingleTPUStrategy)
  strategy: auto

  # The devices to use. Can be set to a positive number (int or str), a sequence of device indices
  # (list or str), the value ``-1`` to indicate all available devices should be used, or ``"auto"`` for
  # automatic selection based on the chosen accelerator. Default: ``"auto"``. (type: Union[List[int], str, int], default: auto)
  devices: auto

  # Number of GPU nodes for distributed training.
  # Default: ``1``. (type: int, default: 1)
  num_nodes: 1

  # Double precision (64, '64' or '64-true'), full precision (32, '32' or '32-true'),
  # 16bit mixed precision (16, '16', '16-mixed') or bfloat16 mixed precision ('bf16', 'bf16-mixed').
  # Can be used on CPU, GPU, TPUs, HPUs or IPUs.
  # Default: ``'32-true'``. (type: Union[Literal[64, 32, 16], Literal['16-mixed', 'bf16-mixed', '32-true', '64-true'], Literal['64', '32', '16', 'bf16']], default: 32-true)
  precision: 16-mixed

  # Logger (or iterable collection of loggers) for experiment tracking. A ``True`` value uses
  # the default ``TensorBoardLogger`` if it is installed, otherwise ``CSVLogger``.
  # ``False`` will disable logging. If multiple loggers are provided, local files
  # (checkpoints, profiler traces, etc.) are saved in the ``log_dir`` of he first logger.
  # Default: ``True``. (type: Union[Logger, Iterable[Logger], bool, null], default: null, known subclasses: pytorch_lightning.loggers.logger.DummyLogger, pytorch_lightning.loggers.CometLogger, pytorch_lightning.loggers.CSVLogger, pytorch_lightning.loggers.MLFlowLogger, pytorch_lightning.loggers.NeptuneLogger, pytorch_lightning.loggers.TensorBoardLogger, pytorch_lightning.loggers.WandbLogger)
  logger:

  # Add a callback or list of callbacks.
  # Default: ``None``. (type: Union[List[Callback], Callback, null], default: null, known subclasses: pytorch_lightning.Callback, pytorch_lightning.callbacks.BatchSizeFinder, pytorch_lightning.callbacks.Checkpoint, pytorch_lightning.callbacks.ModelCheckpoint, pytorch_lightning.callbacks.OnExceptionCheckpoint, pytorch_lightning.callbacks.DeviceStatsMonitor, pytorch_lightning.callbacks.EarlyStopping, pytorch_lightning.callbacks.BaseFinetuning, pytorch_lightning.callbacks.BackboneFinetuning, pytorch_lightning.callbacks.GradientAccumulationScheduler, pytorch_lightning.callbacks.LambdaCallback, pytorch_lightning.callbacks.LearningRateFinder, pytorch_lightning.callbacks.LearningRateMonitor, pytorch_lightning.callbacks.ModelSummary, pytorch_lightning.callbacks.RichModelSummary, pytorch_lightning.callbacks.BasePredictionWriter, pytorch_lightning.callbacks.ProgressBar, pytorch_lightning.callbacks.RichProgressBar, pytorch_lightning.callbacks.TQDMProgressBar, pytorch_lightning.callbacks.Timer, pytorch_lightning.callbacks.ModelPruning, pytorch_lightning.callbacks.StochasticWeightAveraging, pytorch_lightning.cli.SaveConfigCallback, __main__.LogPredictionSamplesCallback)
  callbacks:

  # Runs n if set to ``n`` (int) else 1 if set to ``True`` batch(es)
  # of train, val and test to find any bugs (ie: a sort of unit test).
  # Default: ``False``. (type: Union[int, bool], default: False)
  fast_dev_run: false

  # Stop training once this number of epochs is reached. Disabled by default (None).
  # If both max_epochs and max_steps are not specified, defaults to ``max_epochs = 1000``.
  # To enable infinite training, set ``max_epochs = -1``. (type: Optional[int], default: null)
  max_epochs: 20_000

  # Force training for at least these many epochs. Disabled by default (None). (type: Optional[int], default: null)
  min_epochs:

  # Stop training after this number of steps. Disabled by default (-1). If ``max_steps = -1``
  # and ``max_epochs = None``, will default to ``max_epochs = 1000``. To enable infinite training, set
  # ``max_epochs`` to ``-1``. (type: int, default: -1)
  max_steps: -1

  # Force training for at least these number of steps. Disabled by default (``None``). (type: Optional[int], default: null)
  min_steps:

  # Stop training after this amount of time has passed. Disabled by default (``None``).
  # The time duration can be specified in the format DD:HH:MM:SS (days, hours, minutes seconds), as a
  # :class:`datetime.timedelta`, or a dictionary with keys that will be passed to
  # :class:`datetime.timedelta`. (type: Union[str, timedelta, Dict[str, int], null], default: null)
  max_time:

  # How much of training dataset to check (float = fraction, int = num_batches).
  # Default: ``1.0``. (type: Union[int, float, null], default: null)
  limit_train_batches:

  # How much of validation dataset to check (float = fraction, int = num_batches).
  # Default: ``1.0``. (type: Union[int, float, null], default: null)
  limit_val_batches:

  # How much of test dataset to check (float = fraction, int = num_batches).
  # Default: ``1.0``. (type: Union[int, float, null], default: null)
  limit_test_batches:

  # How much of prediction dataset to check (float = fraction, int = num_batches).
  # Default: ``1.0``. (type: Union[int, float, null], default: null)
  limit_predict_batches:

  # Overfit a fraction of training/validation data (float) or a set number of batches (int).
  # Default: ``0.0``. (type: Union[int, float], default: 0.0)
  overfit_batches: 1

  # How often to check the validation set. Pass a ``float`` in the range [0.0, 1.0] to check
  # after a fraction of the training epoch. Pass an ``int`` to check after a fixed number of training
  # batches. An ``int`` value can only be higher than the number of training batches when
  # ``check_val_every_n_epoch=None``, which validates after every ``N`` training batches
  # across epochs or during iteration-based training.
  # Default: ``1.0``. (type: Union[int, float, null], default: null)
  val_check_interval:

  # Perform a validation loop every after every `N` training epochs. If ``None``,
  # validation will be done solely based on the number of training batches, requiring ``val_check_interval``
  # to be an integer value.
  # Default: ``1``. (type: Optional[int], default: 1)
  check_val_every_n_epoch: 50

  # Sanity check runs n validation batches before starting the training routine.
  # Set it to `-1` to run all batches in all validation dataloaders.
  # Default: ``2``. (type: Optional[int], default: null)
  num_sanity_val_steps:

  # How often to log within steps.
  # Default: ``50``. (type: Optional[int], default: null)
  log_every_n_steps: 1

  # If ``True``, enable checkpointing.
  # It will configure a default ModelCheckpoint callback if there is no user-defined ModelCheckpoint in
  # :paramref:`~pytorch_lightning.trainer.trainer.Trainer.callbacks`.
  # Default: ``True``. (type: Optional[bool], default: null)
  enable_checkpointing:

  # Whether to enable to progress bar by default.
  # Default: ``True``. (type: Optional[bool], default: null)
  enable_progress_bar:

  # Whether to enable model summarization by default.
  # Default: ``True``. (type: Optional[bool], default: null)
  enable_model_summary:

  # Accumulates gradients over k batches before stepping the optimizer.
  # Default: 1. (type: int, default: 1)
  accumulate_grad_batches: 1

  # The value at which to clip gradients. Passing ``gradient_clip_val=None`` disables
  # gradient clipping. If using Automatic Mixed Precision (AMP), the gradients will be unscaled before.
  # Default: ``None``. (type: Union[int, float, null], default: null)
  # gradient_clip_val: 1.0

  # The gradient clipping algorithm to use. Pass ``gradient_clip_algorithm="value"``
  # to clip by value, and ``gradient_clip_algorithm="norm"`` to clip by norm. By default it will
  # be set to ``"norm"``. (type: Optional[str], default: null)
  gradient_clip_algorithm:

  # If ``True``, sets whether PyTorch operations must use deterministic algorithms.
  # Set to ``"warn"`` to use deterministic algorithms whenever possible, throwing warnings on operations
  # that don't support deterministic mode (requires PyTorch 1.11+). If not set, defaults to ``False``.
  # Default: ``None``. (type: Union[bool, Literal['warn'], null], default: null)
  deterministic:

  # The value (``True`` or ``False``) to set ``torch.backends.cudnn.benchmark`` to.
  # The value for ``torch.backends.cudnn.benchmark`` set in the current session will be used
  # (``False`` if not manually set). If :paramref:`~pytorch_lightning.trainer.trainer.Trainer.deterministic`
  # is set to ``True``, this will default to ``False``. Override to manually set a different value.
  # Default: ``None``. (type: Optional[bool], default: null)
  benchmark:

  # Whether to use :func:`torch.inference_mode` or :func:`torch.no_grad` during
  # evaluation (``validate``/``test``/``predict``). (type: bool, default: True)
  inference_mode: true

  # Whether to wrap the DataLoader's sampler with
  # :class:`torch.utils.data.DistributedSampler`. If not specified this is toggled automatically for
  # strategies that require it. By default, it will add ``shuffle=True`` for the train sampler and
  # ``shuffle=False`` for validation/test/predict samplers. If you want to disable this logic, you can pass
  # ``False`` and add your own distributed sampler in the dataloader hooks. If ``True`` and a distributed
  # sampler was already added, Lightning will not replace the existing one. For iterable-style datasets,
  # we don't do this automatically. (type: bool, default: True)
  use_distributed_sampler: true

  # To profile individual steps during training and assist in identifying bottlenecks.
  # Default: ``None``. (type: Union[Profiler, str, null], default: null, known subclasses: pytorch_lightning.profilers.AdvancedProfiler, pytorch_lightning.profilers.PassThroughProfiler, pytorch_lightning.profilers.PyTorchProfiler, pytorch_lightning.profilers.SimpleProfiler, pytorch_lightning.profilers.XLAProfiler)
  profiler:

  # Enable anomaly detection for the autograd engine.
  # Default: ``False``. (type: bool, default: False)
  detect_anomaly: false

  # Whether to run in "barebones mode", where all features that may impact raw speed are
  # disabled. This is meant for analyzing the Trainer overhead and is discouraged during regular training
  # runs. The following features are deactivated:
  # :paramref:`~pytorch_lightning.trainer.trainer.Trainer.enable_checkpointing`,
  # :paramref:`~pytorch_lightning.trainer.trainer.Trainer.logger`,
  # :paramref:`~pytorch_lightning.trainer.trainer.Trainer.enable_progress_bar`,
  # :paramref:`~pytorch_lightning.trainer.trainer.Trainer.log_every_n_steps`,
  # :paramref:`~pytorch_lightning.trainer.trainer.Trainer.enable_model_summary`,
  # :paramref:`~pytorch_lightning.trainer.trainer.Trainer.num_sanity_val_steps`,
  # :paramref:`~pytorch_lightning.trainer.trainer.Trainer.fast_dev_run`,
  # :paramref:`~pytorch_lightning.trainer.trainer.Trainer.detect_anomaly`,
  # :paramref:`~pytorch_lightning.trainer.trainer.Trainer.profiler`,
  # :meth:`~pytorch_lightning.core.module.LightningModule.log`,
  # :meth:`~pytorch_lightning.core.module.LightningModule.log_dict`. (type: bool, default: False)
  barebones: false

  # Plugins allow modification of core behavior like ddp and amp, and enable custom lightning plugins.
  # Default: ``None``. (type: Union[PrecisionPlugin, ClusterEnvironment, CheckpointIO, LayerSync, str, List[Union[PrecisionPlugin, ClusterEnvironment, CheckpointIO, LayerSync, str]], null], default: null, known subclasses: pytorch_lightning.plugins.PrecisionPlugin, pytorch_lightning.plugins.MixedPrecisionPlugin, pytorch_lightning.plugins.FSDPMixedPrecisionPlugin, pytorch_lightning.plugins.DeepSpeedPrecisionPlugin, pytorch_lightning.plugins.DoublePrecisionPlugin, pytorch_lightning.plugins.HPUPrecisionPlugin, pytorch_lightning.plugins.IPUPrecisionPlugin, pytorch_lightning.plugins.TPUPrecisionPlugin, pytorch_lightning.plugins.TPUBf16PrecisionPlugin, lightning_fabric.plugins.environments.KubeflowEnvironment, lightning_fabric.plugins.environments.LightningEnvironment, lightning_fabric.plugins.environments.LSFEnvironment, lightning_fabric.plugins.environments.MPIEnvironment, lightning_fabric.plugins.environments.SLURMEnvironment, lightning_fabric.plugins.environments.TorchElasticEnvironment, lightning_fabric.plugins.environments.XLAEnvironment, lightning_fabric.plugins.TorchCheckpointIO, lightning_fabric.plugins.XLACheckpointIO, pytorch_lightning.plugins.HPUCheckpointIO, pytorch_lightning.plugins.AsyncCheckpointIO, pytorch_lightning.plugins.TorchSyncBatchNorm)
  plugins:

  # Synchronize batch norm layers between process groups/whole world.
  # Default: ``False``. (type: bool, default: False)
  sync_batchnorm: false

  # Set to a non-negative integer to reload dataloaders every n epochs.
  # Default: ``0``. (type: int, default: 0)
  reload_dataloaders_every_n_epochs: 0

  # Default path for logs and weights when no logger/ckpt_callback passed.
  # Default: ``os.getcwd()``.
  # Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/' (type: Union[str, Path, null], default: null)
  default_root_dir:

# <class 'networks.autoencoder.Autoencoder'>
model:

  #   (type: int, default: 32)
  batch_size: 32

  #   (type: int, default: 32)
  test_batch_size: 32

  #   (type: float, default: 0.0001)
  lr: 0.001

  # Used only when output type is POINTCLOUD
  #   (type: int, default: 2025)
  num_points: 2025

  #   (type: int, default: 128)
  emb_dims: 256

  #   (type: str, default: VOXELS)
  input_type: VOXELS

  #   (type: str, default: IMPLICIT)
  output_type: IMPLICIT

  #   (type: float, default: 0.05)
  threshold: 0.05

# Linked arguments
data:

  #   (required, type: str)
  dataset_name: BuildingNet

  #   (required, type: <class 'Path'>)
  dataset_path: "../text2building_data/citydata/buildingnet/"

  # How many points to sample during training when input type is VOXELS. The
  # maximum number of query points is 100_000.
  #   (type: int, default: 5000)
  num_sdf_points: 5000

  #   (type: int, default: 5000)
  test_num_sdf_points: 5000

#   (type: str, default: null)
# ckpt_path: maxdumas/clip_forge_autoencoder/model-njiaypjt:v9
